{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit r/worldnews: Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,precision_score,f1_score,recall_score\n",
    "from numpy import array,asarray,zeros\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.core import Activation, Dense\n",
    "from keras.layers import Flatten, GlobalMaxPooling1D, Conv1D, MaxPooling1D, Concatenate, Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import Callback,EarlyStopping,ModelCheckpoint\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.layers import BatchNormalization\n",
    "from keras import backend as K \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cleaned dataframe\n",
    "comments_final = pd.read_pickle('data/comments_final_.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metric functions\n",
    "from model_nn_metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I investigate performance using a pretrained word embedding framework and convolutional neural network (CNN) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning: Word2Vec Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label target\n",
    "y = comments_final['Removed'].to_numpy()\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     comments_final,y, test_size = 0.2, random_state = 0, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr = X_train['body_clean_stop'].values, y_train\n",
    "x_val, y_val = X_test['body_clean_stop'].values, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize sentences\n",
    "tokenizer = Tokenizer(num_words=100000)\n",
    "\n",
    "# prepare vocabulary\n",
    "tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "# convert text into integer sequences\n",
    "x_tr_seq  = tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq = tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "# pad to get sequences of same length\n",
    "x_tr_seq  = pad_sequences(x_tr_seq, maxlen=300)\n",
    "x_val_seq = pad_sequences(x_val_seq, maxlen=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4529833 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the pretrained embedding into memory\n",
    "# https://wikipedia2vec.github.io/wikipedia2vec/pretrained/\n",
    "w2v_embeddings_index = dict()\n",
    "f = open('data/enwiki_20180420_300d.txt')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    w2v_embeddings_index[word] = coefs\n",
    "\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(w2v_embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132213\n"
     ]
    }
   ],
   "source": [
    "size_of_vocabulary=len(tokenizer.word_index) + 1 # add one for padding\n",
    "\n",
    "# create a weight matrix for words \n",
    "w2v_embedding_matrix = np.zeros((size_of_vocabulary, 300))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    w2v_embedding_vector = w2v_embeddings_index.get(word)\n",
    "    if w2v_embedding_vector is not None:\n",
    "        w2v_embedding_matrix[i] = w2v_embedding_vector\n",
    "print(size_of_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132213, 300)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN: Pretrained Wiki Embeddings, Text-only, 1 Layer, Default Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_51 (Embedding)     (None, 300, 300)          39663900  \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 296, 128)          192128    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_10 (Glo (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_127 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_128 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 39,872,669\n",
      "Trainable params: 208,769\n",
      "Non-trainable params: 39,663,900\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8323 - f1_loss: 0.8319 - f1_beta: 0.2005\n",
      "Epoch 00001: val_f1_loss improved from inf to 0.77253, saving model to models/best_cnn_pretr_emb.h5\n",
      "200/200 [==============================] - 933s 5s/step - loss: 0.8323 - f1_loss: 0.8319 - f1_beta: 0.2005 - val_loss: 0.7722 - val_f1_loss: 0.7725 - val_f1_beta: 0.2495\n",
      "Epoch 2/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7527 - f1_loss: 0.7524 - f1_beta: 0.2443\n",
      "Epoch 00002: val_f1_loss improved from 0.77253 to 0.76961, saving model to models/best_cnn_pretr_emb.h5\n",
      "200/200 [==============================] - 814s 4s/step - loss: 0.7527 - f1_loss: 0.7524 - f1_beta: 0.2443 - val_loss: 0.7693 - val_f1_loss: 0.7696 - val_f1_beta: 0.2725\n",
      "Epoch 3/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7079 - f1_loss: 0.7073 - f1_beta: 0.2745\n",
      "Epoch 00003: val_f1_loss improved from 0.76961 to 0.74533, saving model to models/best_cnn_pretr_emb.h5\n",
      "200/200 [==============================] - 836s 4s/step - loss: 0.7079 - f1_loss: 0.7073 - f1_beta: 0.2745 - val_loss: 0.7448 - val_f1_loss: 0.7453 - val_f1_beta: 0.2176\n",
      "Epoch 4/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6832 - f1_loss: 0.6833 - f1_beta: 0.2909\n",
      "Epoch 00004: val_f1_loss improved from 0.74533 to 0.74120, saving model to models/best_cnn_pretr_emb.h5\n",
      "200/200 [==============================] - 698s 3s/step - loss: 0.6832 - f1_loss: 0.6833 - f1_beta: 0.2909 - val_loss: 0.7409 - val_f1_loss: 0.7412 - val_f1_beta: 0.2782\n",
      "Epoch 5/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6541 - f1_loss: 0.6549 - f1_beta: 0.3073\n",
      "Epoch 00005: val_f1_loss did not improve from 0.74120\n",
      "200/200 [==============================] - 608s 3s/step - loss: 0.6541 - f1_loss: 0.6549 - f1_beta: 0.3073 - val_loss: 0.7694 - val_f1_loss: 0.7700 - val_f1_beta: 0.1778\n",
      "Epoch 6/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6307 - f1_loss: 0.6311 - f1_beta: 0.3230\n",
      "Epoch 00006: val_f1_loss improved from 0.74120 to 0.73328, saving model to models/best_cnn_pretr_emb.h5\n",
      "200/200 [==============================] - 671s 3s/step - loss: 0.6307 - f1_loss: 0.6311 - f1_beta: 0.3230 - val_loss: 0.7328 - val_f1_loss: 0.7333 - val_f1_beta: 0.2254\n",
      "Epoch 7/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6120 - f1_loss: 0.6117 - f1_beta: 0.3390\n",
      "Epoch 00007: val_f1_loss did not improve from 0.73328\n",
      "200/200 [==============================] - 646s 3s/step - loss: 0.6120 - f1_loss: 0.6117 - f1_beta: 0.3390 - val_loss: 0.7465 - val_f1_loss: 0.7470 - val_f1_beta: 0.1971\n",
      "Epoch 8/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5883 - f1_loss: 0.5887 - f1_beta: 0.3534\n",
      "Epoch 00008: val_f1_loss did not improve from 0.73328\n",
      "200/200 [==============================] - 604s 3s/step - loss: 0.5883 - f1_loss: 0.5887 - f1_beta: 0.3534 - val_loss: 0.7586 - val_f1_loss: 0.7590 - val_f1_beta: 0.1823\n",
      "Epoch 9/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5745 - f1_loss: 0.5747 - f1_beta: 0.3620\n",
      "Epoch 00009: val_f1_loss improved from 0.73328 to 0.71723, saving model to models/best_cnn_pretr_emb.h5\n",
      "200/200 [==============================] - 706s 4s/step - loss: 0.5745 - f1_loss: 0.5747 - f1_beta: 0.3620 - val_loss: 0.7170 - val_f1_loss: 0.7172 - val_f1_beta: 0.2538\n",
      "Epoch 10/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5586 - f1_loss: 0.5591 - f1_beta: 0.3724\n",
      "Epoch 00010: val_f1_loss did not improve from 0.71723\n",
      "200/200 [==============================] - 645s 3s/step - loss: 0.5586 - f1_loss: 0.5591 - f1_beta: 0.3724 - val_loss: 0.7294 - val_f1_loss: 0.7296 - val_f1_beta: 0.2188\n",
      "Epoch 11/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5394 - f1_loss: 0.5398 - f1_beta: 0.3839\n",
      "Epoch 00011: val_f1_loss improved from 0.71723 to 0.71719, saving model to models/best_cnn_pretr_emb.h5\n",
      "200/200 [==============================] - 612s 3s/step - loss: 0.5394 - f1_loss: 0.5398 - f1_beta: 0.3839 - val_loss: 0.7170 - val_f1_loss: 0.7172 - val_f1_beta: 0.2547\n",
      "Epoch 12/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5272 - f1_loss: 0.5273 - f1_beta: 0.3925\n",
      "Epoch 00012: val_f1_loss did not improve from 0.71719\n",
      "200/200 [==============================] - 624s 3s/step - loss: 0.5272 - f1_loss: 0.5273 - f1_beta: 0.3925 - val_loss: 0.7346 - val_f1_loss: 0.7349 - val_f1_beta: 0.2100\n",
      "Epoch 13/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5179 - f1_loss: 0.5187 - f1_beta: 0.3977\n",
      "Epoch 00013: val_f1_loss did not improve from 0.71719\n",
      "200/200 [==============================] - 624s 3s/step - loss: 0.5179 - f1_loss: 0.5187 - f1_beta: 0.3977 - val_loss: 0.7417 - val_f1_loss: 0.7419 - val_f1_beta: 0.2007\n",
      "Epoch 14/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5111 - f1_loss: 0.5112 - f1_beta: 0.4031\n",
      "Epoch 00014: val_f1_loss did not improve from 0.71719\n",
      "200/200 [==============================] - 624s 3s/step - loss: 0.5111 - f1_loss: 0.5112 - f1_beta: 0.4031 - val_loss: 0.7431 - val_f1_loss: 0.7433 - val_f1_beta: 0.2002\n",
      "Epoch 00014: early stopping\n",
      "Accuracy: 0.964102\n",
      "Precision: 0.375038\n",
      "Recall: 0.255390\n",
      "F1 score (positive class): 0.303861\n",
      "F1 for both classes: [0.98157596 0.30386052]\n",
      "Weighted Precision: 0.9582139184278449\n",
      "Weighted Recall: 0.9641020042512475\n",
      "Weighted F1: 0.9607856512962558\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(size_of_vocabulary, 300, weights=[w2v_embedding_matrix], input_length=300, trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss=f1_loss, metrics=[f1_loss,f1_beta])\n",
    "\n",
    "# implement early stopping and track val f1 loss\n",
    "es = EarlyStopping(monitor='val_f1_loss', mode='min', verbose=1,patience=3)\n",
    "mc = ModelCheckpoint('models/best_cnn_pretr_emb.h5', monitor='val_f1_loss', mode='min', save_best_only=True,verbose=1)  \n",
    "\n",
    "# print summary of model\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 2500\n",
    "epochs = 20\n",
    "\n",
    "# fit model\n",
    "history = model.fit(np.array(x_tr_seq),np.array(y_tr),batch_size=batch_size,epochs=epochs,validation_data=(np.array(x_val_seq),y_test),verbose=1,callbacks=[mc,es])\n",
    "\n",
    "\n",
    "model = load_model('models/best_cnn_pretr_emb.h5',custom_objects = {'f1_beta':f1_beta,'f1_loss':f1_loss})\n",
    "\n",
    "# predict probabilities for test set\n",
    "yhat_probs = model.predict(x=np.array(x_val_seq), verbose=0)\n",
    "\n",
    "# reduce to 1d array\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "yhat_probs[yhat_probs>=0.5] = 1\n",
    "yhat_probs[yhat_probs<0.5] = 0\n",
    "\n",
    "# accuracy = (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test, yhat_probs)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "\n",
    "# precision (for positive class) = tp / (tp + fp)\n",
    "precision = precision_score(y_test, yhat_probs)\n",
    "print('Precision: %f' % precision)\n",
    "\n",
    "# recall (for positive class) = tp / (tp + fn)\n",
    "recall = recall_score(y_test, yhat_probs)\n",
    "print('Recall: %f' % recall)\n",
    "\n",
    "# f1 (for positive class) = 2 * precision * recall/(precision + recall)\n",
    "f1 = f1_score(y_test, yhat_probs)\n",
    "print('F1 score (positive class): %f' % f1)\n",
    "\n",
    "# f1 for both classes\n",
    "f1_both = f1_score(y_test,yhat_probs,average=None)\n",
    "print(f'F1 for both classes: {f1_both}')\n",
    "\n",
    "# weighted precision\n",
    "precision_w = precision_score(y_test,yhat_probs,average='weighted')\n",
    "print(f'Weighted Precision: {precision_w}')\n",
    "\n",
    "# weighted recall\n",
    "recall_w = recall_score(y_test,yhat_probs,average='weighted')\n",
    "print(f'Weighted Recall: {recall_w}')\n",
    "\n",
    "# weighted F1\n",
    "f1_w = f1_score(y_test,yhat_probs,average='weighted')\n",
    "print(f'Weighted F1: {f1_w}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained w2v: Text + Non-text features (Wikipedia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'score', 'subreddit', 'parent_id', 'id', 'created_utc',\n",
       "       'Removed', 'body', 'author', 'body_no_quotes', 'body_norm',\n",
       "       'body_norm_mod', 'body_clean_no_stop', 'body_clean_stop', 'run_rem',\n",
       "       'run_tot', 'run_prop_rem', 'run_prev_rem', 'run_prev_tot',\n",
       "       'run_prop_prev_rem', 'parent_id_2', 'parent_prefix', 'child_rem_flag',\n",
       "       'sec_child_rem_flag', 'third_child_rem_flag', 'fourth_child_rem_flag',\n",
       "       'fifth_child_rem_flag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify final set of text & non-text features\n",
    "final_features = ['body_clean_stop','run_prop_prev_rem','child_rem_flag','sec_child_rem_flag','third_child_rem_flag', 'fourth_child_rem_flag','fifth_child_rem_flag','run_prev_rem','run_prev_tot']\n",
    "X = comments_final[final_features]\n",
    "\n",
    "y = comments_final['Removed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text features\n",
    "X1_train = X_train['body_clean_stop']\n",
    "X1_test = X_test['body_clean_stop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize sentences\n",
    "tokenizer = Tokenizer(num_words=100000)\n",
    "tokenizer.fit_on_texts(X1_train)\n",
    "\n",
    "# convert text into integer sequences\n",
    "X1_train = tokenizer.texts_to_sequences(X1_train)\n",
    "X1_test = tokenizer.texts_to_sequences(X1_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1 # add one for padding\n",
    "\n",
    "maxlen = 200\n",
    "\n",
    "# pad to get sequences of same length\n",
    "X1_train = pad_sequences(X1_train, padding='post', maxlen=maxlen)\n",
    "X1_test = pad_sequences(X1_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split non-text features\n",
    "X2_train = X_train[['run_prop_prev_rem','child_rem_flag','sec_child_rem_flag','third_child_rem_flag', 'fourth_child_rem_flag','fifth_child_rem_flag','run_prev_rem','run_prev_tot']].values\n",
    "X2_test = X_test[['run_prop_prev_rem','child_rem_flag','sec_child_rem_flag','third_child_rem_flag', 'fourth_child_rem_flag','fifth_child_rem_flag','run_prev_rem','run_prev_tot']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize non-text features\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X2_train_s = scaler.fit_transform(X2_train)\n",
    "X2_test_s = scaler.transform(X2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN: Non-text features & Pretrained w2v embeddings, 1 Layer, Default Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_81\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_83 (InputLayer)           [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_68 (Embedding)        (None, 200, 300)     39663900    input_83[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 196, 128)     192128      embedding_68[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_25 (Global (None, 128)          0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_84 (InputLayer)           [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_40 (Concatenate)    (None, 136)          0           global_max_pooling1d_25[0][0]    \n",
      "                                                                 input_84[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_176 (Dense)               (None, 128)          17536       concatenate_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_177 (Dense)               (None, 64)           8256        dense_176[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_178 (Dense)               (None, 1)            65          dense_177[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 39,881,885\n",
      "Trainable params: 217,985\n",
      "Non-trainable params: 39,663,900\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.6557 - f1_loss: 0.6552 - f1_beta: 0.3342\n",
      "Epoch 00001: val_f1_loss improved from inf to 0.52905, saving model to models/best_cnn_w2v_emb_nontext.h5\n",
      "250/250 [==============================] - 538s 2s/step - loss: 0.6557 - f1_loss: 0.6552 - f1_beta: 0.3342 - val_loss: 0.5301 - val_f1_loss: 0.5291 - val_f1_beta: 0.4069\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.5021 - f1_loss: 0.5027 - f1_beta: 0.4568\n",
      "Epoch 00002: val_f1_loss did not improve from 0.52905\n",
      "250/250 [==============================] - 548s 2s/step - loss: 0.5021 - f1_loss: 0.5027 - f1_beta: 0.4568 - val_loss: 0.5331 - val_f1_loss: 0.5325 - val_f1_beta: 0.3854\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4689 - f1_loss: 0.4693 - f1_beta: 0.4793\n",
      "Epoch 00003: val_f1_loss did not improve from 0.52905\n",
      "250/250 [==============================] - 567s 2s/step - loss: 0.4689 - f1_loss: 0.4693 - f1_beta: 0.4793 - val_loss: 0.5388 - val_f1_loss: 0.5383 - val_f1_beta: 0.4874\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4527 - f1_loss: 0.4545 - f1_beta: 0.4905\n",
      "Epoch 00004: val_f1_loss improved from 0.52905 to 0.50805, saving model to models/best_cnn_w2v_emb_nontext.h5\n",
      "250/250 [==============================] - 541s 2s/step - loss: 0.4527 - f1_loss: 0.4545 - f1_beta: 0.4905 - val_loss: 0.5086 - val_f1_loss: 0.5081 - val_f1_beta: 0.4851\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4328 - f1_loss: 0.4339 - f1_beta: 0.5032\n",
      "Epoch 00005: val_f1_loss did not improve from 0.50805\n",
      "250/250 [==============================] - 534s 2s/step - loss: 0.4328 - f1_loss: 0.4339 - f1_beta: 0.5032 - val_loss: 0.5085 - val_f1_loss: 0.5081 - val_f1_beta: 0.4257\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4251 - f1_loss: 0.4246 - f1_beta: 0.5147\n",
      "Epoch 00006: val_f1_loss improved from 0.50805 to 0.50008, saving model to models/best_cnn_w2v_emb_nontext.h5\n",
      "250/250 [==============================] - 533s 2s/step - loss: 0.4251 - f1_loss: 0.4246 - f1_beta: 0.5147 - val_loss: 0.5008 - val_f1_loss: 0.5001 - val_f1_beta: 0.4689\n",
      "Epoch 7/20\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4118 - f1_loss: 0.4126 - f1_beta: 0.5239\n",
      "Epoch 00007: val_f1_loss improved from 0.50008 to 0.49954, saving model to models/best_cnn_w2v_emb_nontext.h5\n",
      "250/250 [==============================] - 530s 2s/step - loss: 0.4118 - f1_loss: 0.4126 - f1_beta: 0.5239 - val_loss: 0.5000 - val_f1_loss: 0.4995 - val_f1_beta: 0.4391\n",
      "Epoch 8/20\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4070 - f1_loss: 0.4064 - f1_beta: 0.5283\n",
      "Epoch 00008: val_f1_loss did not improve from 0.49954\n",
      "250/250 [==============================] - 526s 2s/step - loss: 0.4070 - f1_loss: 0.4064 - f1_beta: 0.5283 - val_loss: 0.5032 - val_f1_loss: 0.5027 - val_f1_beta: 0.4328\n",
      "Epoch 9/20\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3943 - f1_loss: 0.3942 - f1_beta: 0.5385\n",
      "Epoch 00009: val_f1_loss improved from 0.49954 to 0.49626, saving model to models/best_cnn_w2v_emb_nontext.h5\n",
      "250/250 [==============================] - 533s 2s/step - loss: 0.3943 - f1_loss: 0.3942 - f1_beta: 0.5385 - val_loss: 0.4966 - val_f1_loss: 0.4963 - val_f1_beta: 0.4571\n",
      "Epoch 10/20\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3864 - f1_loss: 0.3860 - f1_beta: 0.5432\n",
      "Epoch 00010: val_f1_loss did not improve from 0.49626\n",
      "250/250 [==============================] - 525s 2s/step - loss: 0.3864 - f1_loss: 0.3860 - f1_beta: 0.5432 - val_loss: 0.5133 - val_f1_loss: 0.5129 - val_f1_beta: 0.4089\n",
      "Epoch 11/20\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3784 - f1_loss: 0.3782 - f1_beta: 0.5480\n",
      "Epoch 00011: val_f1_loss did not improve from 0.49626\n",
      "250/250 [==============================] - 522s 2s/step - loss: 0.3784 - f1_loss: 0.3782 - f1_beta: 0.5480 - val_loss: 0.5207 - val_f1_loss: 0.5209 - val_f1_beta: 0.4798\n",
      "Epoch 12/20\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3826 - f1_loss: 0.3827 - f1_beta: 0.5465\n",
      "Epoch 00012: val_f1_loss did not improve from 0.49626\n",
      "250/250 [==============================] - 500s 2s/step - loss: 0.3826 - f1_loss: 0.3827 - f1_beta: 0.5465 - val_loss: 0.5036 - val_f1_loss: 0.5029 - val_f1_beta: 0.4703\n",
      "Epoch 13/20\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3786 - f1_loss: 0.3792 - f1_beta: 0.5485\n",
      "Epoch 00013: val_f1_loss did not improve from 0.49626\n",
      "250/250 [==============================] - 538s 2s/step - loss: 0.3786 - f1_loss: 0.3792 - f1_beta: 0.5485 - val_loss: 0.4987 - val_f1_loss: 0.4987 - val_f1_beta: 0.4504\n",
      "Epoch 14/20\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3694 - f1_loss: 0.3693 - f1_beta: 0.5571\n",
      "Epoch 00014: val_f1_loss did not improve from 0.49626\n",
      "250/250 [==============================] - 583s 2s/step - loss: 0.3694 - f1_loss: 0.3693 - f1_beta: 0.5571 - val_loss: 0.5027 - val_f1_loss: 0.5025 - val_f1_beta: 0.4423\n",
      "Epoch 00014: early stopping\n",
      "Accuracy: 0.974184\n",
      "Precision: 0.612951\n",
      "Recall: 0.429977\n",
      "F1 score (positive class): 0.505413\n",
      "F1 for both classes: [0.98674623 0.50541339]\n",
      "Weighted Precision: 0.9708035177259803\n",
      "Weighted Recall: 0.9741842678157451\n",
      "Weighted F1: 0.9719803600061895\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "input_1 = Input(shape=(maxlen,))\n",
    "input_2 = Input(shape=(8,))\n",
    "embedding_layer = Embedding(size_of_vocabulary, 300, weights=[w2v_embedding_matrix], trainable=False)(input_1)\n",
    "Conv_1 = Conv1D(128, 5, activation='relu')(embedding_layer)\n",
    "Conv_2 = GlobalMaxPooling1D()(Conv_1)\n",
    "concat_layer = Concatenate()([Conv_2, input_2])\n",
    "dense_layer = Dense(128, activation='relu')(concat_layer)\n",
    "dense_layer2 = Dense(64, activation='relu')(dense_layer)\n",
    "output = Dense(1, activation='sigmoid')(dense_layer2)\n",
    "model = Model(inputs=[input_1, input_2], outputs=output)\n",
    "\n",
    "# print summary of model\n",
    "model.compile(loss=f1_loss, optimizer='adam', metrics=[f1_loss,f1_beta])\n",
    "print(model.summary())\n",
    "\n",
    "# implement early stopping and track val f1 loss\n",
    "es = EarlyStopping(monitor='val_f1_loss', mode='min', verbose=1,patience=5)  \n",
    "mc = ModelCheckpoint('models/best_cnn_w2v_emb_nontext.h5', monitor='val_f1_loss', mode='min', save_best_only=True,verbose=1)  \n",
    "\n",
    "batch_size = 2500\n",
    "epochs = epochs\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(x=[X1_train, X2_train_s], y=y_train, validation_data=([X1_test,X2_test_s],y_test), epochs=epochs, batch_size=batch_size, verbose=1,callbacks=[es,mc])\n",
    "\n",
    "model = load_model('models/best_cnn_w2v_emb_nontext.h5',custom_objects = {'f1_beta':f1_beta,'f1_loss':f1_loss})\n",
    "\n",
    "# predict probabilities for test data\n",
    "yhat_probs = model.predict(x=[X1_test,X2_test_s], verbose=0)\n",
    "\n",
    "# reduce to 1d array\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "yhat_probs[yhat_probs>=0.5] = 1\n",
    "yhat_probs[yhat_probs<0.5] = 0\n",
    " \n",
    "# accuracy = (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test, yhat_probs)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "\n",
    "# precision (for positive class) = tp / (tp + fp)\n",
    "precision = precision_score(y_test, yhat_probs)\n",
    "print('Precision: %f' % precision)\n",
    "\n",
    "# recall (for positive class) = tp / (tp + fn)\n",
    "recall = recall_score(y_test, yhat_probs)\n",
    "\n",
    "print('Recall: %f' % recall)\n",
    "\n",
    "# f1 (for positive class) = 2 * precision * recall/(precision + recall)\n",
    "f1 = f1_score(y_test, yhat_probs)\n",
    "print('F1 score (positive class): %f' % f1)\n",
    "\n",
    "# f1 (for both classes)\n",
    "f1_both = f1_score(y_test,yhat_probs,average=None)\n",
    "print(f'F1 for both classes: {f1_both}')\n",
    "\n",
    "# weighted precision\n",
    "precision_w = precision_score(y_test,yhat_probs,average='weighted')\n",
    "print(f'Weighted Precision: {precision_w}')\n",
    "\n",
    "# weighted recall\n",
    "recall_w = recall_score(y_test,yhat_probs,average='weighted')\n",
    "print(f'Weighted Recall: {recall_w}')\n",
    "\n",
    "# weighted F1\n",
    "f1_w = f1_score(y_test,yhat_probs,average='weighted')\n",
    "print(f'Weighted F1: {f1_w}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For text-only features,  using a CNN and pretrained embedding layer, we get a weighted F1 of 0.961 and F1 (positive class) of 0.304. Compared to the best text-only TF-IDF model (random forest using bigrams with weighted F1 of 0.960 and F1 (positive class) of 0.201), these figures represent a 0.1% improvement for weighted F1 and 51.2% improvement for F1 (positive class), respectively. For text and non-text features, also using a CNN and pretrained embedding layer, we get a weighted F1 of 0.972 and F1 (positive class) of 0.505. These models outperform my \"best\" text- and non-text TF-IDF model (logistic regression using unigrams, where weighted F1 is 0.971, and F1 for the positive class is 0.444) by 0.1% and 13.7%, respectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
